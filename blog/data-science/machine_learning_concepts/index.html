<!DOCTYPE html>
<html lang="en">



<head>
    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Eunice Koid | Machine Learning Concepts</title>
    <meta name="author" content="Eunice Koid" />
    <meta name="description" content="" />
    <meta name="keywords" content="data-science, statistics" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css"
        integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700&display=swap">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"
        integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css"
        integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css"
        media="none" id="highlight_theme_light" />

    <!-- Styles -->

    <link rel="shortcut icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>◼️</text></svg>">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://eunicekoid.github.io/">

    <!-- Dark Mode -->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css"
        media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    <style>
        body {
            margin-left: 2px;
        }

        .fixedNavigation {
            position: fixed;
            top: 1000;
            width: 275px;
            left: 10;
            padding: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            font-size: 15px;
        }

        .submenu {
            margin-left: 3px;
            font-size: 11px;
        }
    </style>

</head>

<!-- Body -->

<body class="fixed-top-nav ">

    <!-- Header -->
    <header>

        <!-- Nav Bar -->
        <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
            <div class="container">
                <a class="navbar-brand title font-weight-lighter" href="https://eunicekoid.github.io/"><span
                        class="font-weight-bold">Eunice</span> Koid</a>
                <!-- Navbar Toggle -->
                <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse"
                    data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
                    aria-label="Toggle navigation">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar top-bar"></span>
                    <span class="icon-bar middle-bar"></span>
                    <span class="icon-bar bottom-bar"></span>
                </button>

                <div class="collapse navbar-collapse text-right" id="navbarNav">
                    <ul class="navbar-nav ml-auto flex-nowrap">

                        <!-- About -->
                        <li class="nav-item ">
                            <a class="nav-link" href="/">about</a>
                        </li>

                        <!-- Portfolio -->
                        <li class="nav-item ">
                            <a class="nav-link" href="/portfolio/">portfolio</a>
                        </li>

                        <!-- Blog -->
                        <li class="nav-item ">
                            <a class="nav-link" href="/blog/">blog</a>
                        </li>

                        <!-- Toogle theme mode -->
                        <div class="toggle-container">
                            <a id="light-toggle">
                                <i class="fas fa-moon"></i>
                                <i class="fas fa-sun"></i>
                            </a>
                        </div>
                    </ul>
                </div>
            </div>
        </nav>
    </header>



    <!-- Content -->
    <div class="container mt-5">
        <!-- page.html -->
        <div class="post">

            <header class="post-header">
                <h1 class="post-title">Machine Learning Concepts</h1>
                <p class="post-description">Click on each term to view its definition. </p>
            </header>
            <hr>


            <article>
                <style type="text/css">
                    .tg {
                        border-collapse: collapse;
                        border-spacing: 0;
                    }

                    .tg td {
                        border-color: #d3d3d3;
                        border-style: solid;
                        border-width: 1px;
                        font-size: 13px;
                        overflow: hidden;
                        padding: 5px 6px;
                        word-break: normal;
                    }

                    .tg th {
                        border-color: #d3d3d3;
                        border-style: solid;
                        border-width: 1px;
                        font-size: 13px;
                        overflow: hidden;
                        padding: 5px 6px;
                        word-break: normal;
                    }

                    .tg .tg-fymr {
                        border-color: #d3d3d3;
                        font-weight: bold;
                        text-align: left;
                        vertical-align: top
                    }

                    .tg .tg-0pky {
                        border-color: #d3d3d3;
                        text-align: left;
                        vertical-align: top
                    }
                </style>


                <!-- Button -->
                <button id="acc-button">accuracy</button>
                <p id="acc-paragraph" style="display: none;">
                    A classification model evaluation metric. \( \text{Accuracy} = \frac{\text{number of correct predictions}}{\text{total number of predictions}} \)
                    <br><br>
                    <small>(not good for imbalanced datasets)</small>
                </p>
                <script>
                    const accButton = document.getElementById('acc-button');
                    const accPara = document.getElementById('acc-paragraph');
                    accButton.addEventListener('click', () => {
                        if (accPara.style.display === 'none') {
                            accPara.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            accPara.style.display = 'none';
                        }
                    });
                </script>

                <!-- Button -->
                <button id="ada-button">ada boost</button>
                <p id="ada-paragraph" style="display: none;">
                    Adaptive boosting is an ensemble method that sequentially combines decision trees with a single
                    split and sets weights uniformly for all data points.
                    The data points are reweighted at each iteration depending on whether it was correctly classfied or
                    not. The weighted predictions of each classifier are combined at the end to obtain a final
                    prediction.
                </p>
                <script>
                    const adaB = document.getElementById('ada-button');
                    const adaP = document.getElementById('ada-paragraph');
                    adaB.addEventListener('click', () => {
                        if (adaP.style.display === 'none') {
                            adaP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            adaP.style.display = 'none';
                        }
                    });
                </script>


                <br />
                <br />

                <!-- Button -->
                <button id="backpropogation-button">backpropogation</button>
                <p id="backpropogation-paragraph" style="display: none;">
                    An algorithm that efficiently calculates the gradient of the cost function of the neural network
                    with respect to the parameters. The gradients are used to update the parameters, thereby improving
                    the model.
                </p>
                <script>
                    const backpropogationB = document.getElementById('backpropogation-button');
                    const backpropogationP = document.getElementById('backpropogation-paragraph');
                    backpropogationB.addEventListener('click', () => {
                        if (backpropogationP.style.display === 'none') {
                            backpropogationP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            backpropogationP.style.display = 'none';
                        }
                    });
                </script>


                <!-- Button -->
                <button id="bagging-button">bagging</button>
                <p id="bagging-paragraph" style="display: none;">
                    Short for Bootstrap Aggregating, bagging is a machine learning ensemble technique designed to
                    improve the stability and accuracy of machine learning algorithms.
                    Multiple models (often called "weak learners") are trained and combined to produce a stronger
                    overall model.
                    Multiple subsets of the original training data are created using bootstrap sampling (randomly
                    sampling with replacement).
                    A separate model is trained on each of these bootstrapped subsets. Since each subset is different,
                    each model will have slight variations.
                    Once all the models are trained, their predictions are combined. For regression tasks, this is
                    usually done by averaging the predictions. For classification tasks, it is often done by majority
                    voting.
                </p>
                <script>
                    const baggingB = document.getElementById('bagging-button');
                    const baggingP = document.getElementById('bagging-paragraph');
                    baggingB.addEventListener('click', () => {
                        if (baggingP.style.display === 'none') {
                            baggingP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            baggingP.style.display = 'none';
                        }
                    });
                </script>

                <!-- Button -->
                <button id="bv-button">bias-variance tradeoff</button>
                <p id="bv-paragraph" style="display: none;">
                    Bias is how close the model's predictions are to actual data. A high bias model oversimplifies the
                    data and therefore underfits the data. <br><br>

                    Variance is the model's sensitivity to fluctuations - how much prediction error changes based on
                    training inputs. High variance models overfit the data; they tend to perform well on training set
                    but not on validation or test sets. To counter overfitting (reduce variance: <br>
                    • feature selection<br>
                    • regulatization<br>
                    • dimensionality reduction<br>
                    • subset selection (best subset selection, stepwise selection) <br><br>

                    When modeling, the tradeoff between high bias/low variance and high variance/low bias needs to be
                    balanced.
                </p>
                <script>
                    const bvB = document.getElementById('bv-button');
                    const bvP = document.getElementById('bv-paragraph');
                    bvB.addEventListener('click', () => {
                        if (bvP.style.display === 'none') {
                            bvP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            bvP.style.display = 'none';
                        }
                    });
                </script>



                <!-- Button -->
                <button id="boosting-button">boosting</button>
                <p id="boosting-paragraph" style="display: none;">
                    Boosting is An ensemble learning technique that combines the predictions of multiple weak learners
                    (models that are slightly better than random guessing) to produce a strong learner with better
                    performance.
                    Boosting trains weak learners <i>sequentially</i> where each subsequent learner focuses on
                    addressing the mistakes of the previous model.
                </p>
                <script>
                    const boostingB = document.getElementById('boosting-button');
                    const boostingP = document.getElementById('boosting-paragraph');
                    boostingB.addEventListener('click', () => {
                        if (boostingP.style.display === 'none') {
                            boostingP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            boostingP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />



                <!--Button-->
                <button id="classstypes-button">classification model types</button>
                <div id="classstypes-paragraph" style="display: none; text-align: left;">

                    <div style="margin: 0 auto; padding: 10px; display: inline-block;">
                        <table border=".1">
                            <tr>
                                <td><b>&nbsp;Generative Models</b></td>
                                <td><b>&nbsp;Discriminative Models</b></td>
                            </tr>
                            <tr>
                                <td>&nbsp;Joint Distribution of \(X\) and \(Y\) &nbsp;<br> &nbsp;\(P(X,Y) = P(Y|X)P(X)
                                    \)
                                </td>
                                <td>&nbsp;\( \hat{y} = argmax_k P(Y = k|x) \)&nbsp; </td>

                            </tr>

                        </table>
                        <br>
                        Determine decision boundary between classes by maximizing the posterior probability
                        distribution.
                    </div>
                </div>
                <script>
                    const classstypesB = document.getElementById('classstypes-button');
                    const classstypesP = document.getElementById('classstypes-paragraph');

                    classstypesB.addEventListener('click', () => {
                        if (classstypesP.style.display === 'none') {
                            classstypesP.style.display = 'block';
                        } else {
                            classstypesP.style.display = 'none';
                        }
                    });
                </script>
                <!-- Button -->
                <button id="clustering-button">clustering</button>
                <p id="clustering-paragraph" style="display: none;">
                    An unsupervised machine learning approach to group comparable data points by certain traits.<br>
                    Points in a cluster have high intra-cluster similarity. Points in different clusters have low
                    inter-cluster similarity.
                    <br><br>

                    <b>K-means Clustering</b><br>
                    &nbsp; • Partitions data into <i>k</i> clusters and then arbitrarily selects centroids of those
                    clusters
                    <br>
                    &nbsp; • Updates the groups by assigning points to closest cluster, updating the centroids, and then
                    repeat
                    until convergence<br><br>

                    <b>Hierarchical Clustering</b><br>
                    &nbsp; • Assigns data points to clusters and adds nearest points until there's one cluster left <br>
                    &nbsp; • Dendogram - more interpretable <br><br>

                    <b>Density Clustering (DBSCAN)</b><br>
                    &nbsp; • Groups points together that have high density - have many neighbors <br>
                    &nbsp; • Good at detecting outliers
                    <br><br>

                    <b>Gaussian Mixture Models (GMM)</b><br>
                    &nbsp; • Clusters data points based on their distribution<br>
                    &nbsp; • Assumes the data comes from a mixture of <i>k</i> Gaussian distributions with different
                    means and variances.
                    &nbsp;&nbsp;&nbsp;&nbsp; Each Gaussian distribution in the mixture represents a cluster within the
                    data. <br>
                    &nbsp; • Estimates parameters (e.g., mean, variance) of these Gaussian distributions to best fit the
                    data <br>
                    &nbsp; • Set <i>k</i> in advance; GMM tries to learn the true value of <i>k</i> <br>
                    &nbsp; • More flexible than k-means (takes into account mean and variance); low dimensional data
                    with arbitrary shapes






                </p>
                <script>
                    const clusteringB = document.getElementById('clustering-button');
                    const clusteringP = document.getElementById('clustering-paragraph');
                    clusteringB.addEventListener('click', () => {
                        if (clusteringP.style.display === 'none') {
                            clusteringP.style.display = 'block';
                        } else {
                            clusteringP.style.display = 'none';
                        }
                    });
                </script>


                <!--Button-->
                <button id="r2-button">coefficient of determination (R<sup>2</sup>)</button>
                <p id="r2-paragraph" style="display: none;">
                    R<sup>2</sup> measures how well the independent variables explain the variability in the dependent
                    variable (the proportion of variability explained by the model). <br><br>

                    If R<sup>2</sup>=1, the model perfectly predicts the dependent variable. <br>
                    If R<sup>2</sup>=0, the model does not explain any variability in the dependent variable. <br><br>

                    R<sup>2</sup> = SSR (explained variance) / SST (total variance)
                    <br><br>
                    R<sup>2</sup> only works for simple linear regression. For multiple linear regression, R<sup>2</sup>
                    increases as the number of independent variables increases, even if the independent variable is
                    insignificant. In this case, use the Adjusted R-Squared metric.
                    <br><br>
                    \( Adjusted R^2 = 1 - \frac{(1-R^2)(N-1)}{N-p-1} \) <br>
                    &nbsp;&nbsp;where R<sup>2</sup> is the sample R-squared value <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; N is the total sample size
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p is the number of
                    independent variables
                    <br><br>
                    Generally, R<sup>2</sup> increases when you add more features, but that does not mean it's a better
                    model. It could be overfitted.
                </p>
                <script>
                    const r2Button = document.getElementById('r2-button');
                    const r2Para = document.getElementById('r2-paragraph');
                    r2Button.addEventListener('click', () => {
                        if (r2Para.style.display === 'none') {
                            r2Para.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            r2Para.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="confounding-button">confounding</button>
                <p id="confounding-paragraph" style="display: none;">

                    When a variable affects the relationship between dependent & independent variables.
                    Causes invalid correlations (e.g., multicollinearity).
                    <br><br>
                    Can happen when:<br>
                    - there is selection bias in data collection (e.g. group imbalance) <br>
                    - there is omitted variable bias either during choices made during
                    modeling or data generation issues
                    <br><br>
                    Address with:<br>
                    - Stratification <br>
                    - Chi-square test (test associations & significance)
                </p>
                <script>
                    const confoundingB = document.getElementById('confounding-button');
                    const confoundingP = document.getElementById('confounding-paragraph');

                    confoundingB.addEventListener('click', () => {
                        if (confoundingP.style.display === 'none') {
                            confoundingP.style.display = 'block';
                        } else {
                            confoundingP.style.display = 'none';
                        }
                    });
                </script>


                <!--Button-->
                <button id="confusion-button">confusion matrix</button>
                <p id="confusion-paragraph" style="display: none;">
                    <img class="img-fluid z-dept-1" src="images/confusion_matrix.png" alt="confusion_matrix.png"
                        style="width: 90%;" />

                </p>
                <script>
                    const confusionButton = document.getElementById('confusion-button');
                    const confusionParagraph = document.getElementById('confusion-paragraph');

                    confusionButton.addEventListener('click', () => {
                        if (confusionParagraph.style.display === 'none') {
                            confusionParagraph.style.display = 'block';
                        } else {
                            confusionParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="cook-button">cook's distance</button>
                <p id="cook-paragraph" style="display: none;">
                    Estimates the influence of any given data point. Considers residual and leverage (how far the point
                    difference from other X values). A method to identify outliers; remove points beyond a certain
                    threshold
                </p>
                <script>
                    const cookB = document.getElementById('cook-button');
                    const cookP = document.getElementById('cook-paragraph');

                    cookB.addEventListener('click', () => {
                        if (cookP.style.display === 'none') {
                            cookP.style.display = 'block';
                        } else {
                            cookP.style.display = 'none';
                        }
                    });
                </script>


                <!--Button-->
                <button id="cv-button">cross validation</button>
                <p id="cv-paragraph" style="display: none;">
                    Cross validation is a method of splitting the data into training and test subsets to avoid testing
                    and training on the same data. Doing so may cause overfitting which decrease the model's
                    performance. <br><br>

                    <b>k-fold cross validation</b><br>
                    1. Randomly divide the data into <i>folds</i> of equal size <br>
                    2. Train the model on all folds except for one, which is the validation set on which the model is
                    evaluated. <br>
                    &nbsp; &nbsp; For each training run, change the validation fold. <br>
                    3. Average the <i>k</i> validation errors to get an estimate of the true error.
                    <br><br>

                    <b>leave one out corss validation (LOOCV)</b> <br>
                    A special case of k-fold cross validation where <i>k</i> equals the size of the dataset (<i>n</i>).
                    The model tests every single data point during cross validation. Computationally expensive.
                    <br><br>
                    <b>Cross validation for time series data</b><br>
                    Time series data is not randomly distributed so standard k-fold CV cannot be used. Data from the
                    future cannot be used to train data from the past. Use historical data until a given point in time,
                    and vary that point in time from beginning to end.
                </p>
                <script>
                    const cvB = document.getElementById('cv-button');
                    const cvP = document.getElementById('cv-paragraph');

                    cvB.addEventListener('click', () => {
                        if (cvP.style.display === 'none') {
                            cvP.style.display = 'block';
                        } else {
                            cvP.style.display = 'none';
                        }
                    });
                </script>
                <br />
                <br />

                <!--Button-->
                <button id="decisiontree-button">decision trees</button>
                <p id="decisiontree-paragraph" style="display: none;"> A supervised machine learning model that can be
                    used for classification or regression problems. The algorithm is trained in a greedy and recursive
                    fashion starting at a root node and making binary splits in features that lead to minimal error.
                </p>
                <script>
                    const decisiontreeB = document.getElementById('decisiontree-button');
                    const decisiontreeP = document.getElementById('decisiontree-paragraph');

                    decisiontreeB.addEventListener('click', () => {
                        if (decisiontreeP.style.display === 'none') {
                            decisiontreeP.style.display = 'block';
                        } else {
                            decisiontreeP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />

                <!--Button-->
                <button id="entropy-button">entropy</button>
                <p id="entropy-paragraph" style="display: none;"> Quantifies uncertainty in a random variable assuming
                    \(k\) states <br>
                    Entropy \(H(Y) = \sum_{i=1}^k P(Y = k) log(PY=k)\) <br>
                    Higher entropy means closer to uniform distribution than a skewed one.<br><br>
                    Example: Gini index
                </p>
                <script>
                    const entropyB = document.getElementById('entropy-button');
                    const entropyP = document.getElementById('entropy-paragraph');

                    entropyB.addEventListener('click', () => {
                        if (entropyP.style.display === 'none') {
                            entropyP.style.display = 'block';
                        } else {
                            entropyP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />



                <!--Button-->
                <button id="f1-button">F1 score</button>
                <p id="f1-paragraph" style="display: none;">A classification model evaluation metric, good for
                    imbalanced datasets>
                    \[ F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}} \]
                    Optimize for the F1 score to balance precision vs. recall.
                </p>
                <script>
                    const f1Button = document.getElementById('f1-button');
                    const f1Paragraph = document.getElementById('f1-paragraph');

                    f1Button.addEventListener('click', () => {
                        if (f1Paragraph.style.display === 'none') {
                            f1Paragraph.style.display = 'block';
                        } else {
                            f1Paragraph.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />




                <!--Button-->
                <button id="glm-button">generalized linear models (GLM)</button>
                <p id="glm-paragraph" style="display: none;">Linear regression that allows residuals to not be normally
                    distributed
                    <br>
                    <img class="img-fluid z-dept-1" src="images/glm.png" alt="glm.png" style="width: 70%;" />

                </p>
                <script>
                    const glmB = document.getElementById('glm-button');
                    const glmP = document.getElementById('glm-paragraph');

                    glmB.addEventListener('click', () => {
                        if (glmP.style.display === 'none') {
                            glmP.style.display = 'block';
                        } else {
                            glmP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="gd-button">gradient descent</button>
                <p id="gd-paragraph" style="display: none;">
                    An optimization algorithm used to minimize the cost function (error) in machine learning models.
                    <br>
                    It starts by taking an initial guess of model parameters. Then, it iteratively adjusts the
                    parameters by taking small steps in the direct of steepest descent of the cost function. The steps
                    are determined by the hyperparameter learning rate (\(\alpha\)).
                    At each iteration, gradient descent calculates the gradient of the cost function with respect to
                    each parameter. The gradient indicates the direction and rate of fastest increase of the cost
                    function. It then updates the parameters in the opposite direction of the gradient (because we want
                    to minimize the cost function), scaled by the learning rate.
                    This process continues iteratively until the algorithm converges to a point where further
                    adjustments to the parameters do not significantly reduce the cost function anymore.<br><br>

                    \( x_{t+1} = x_t - \alpha_t \nabla f(x_t) \)<br>
                    &nbsp; where \(f(x_t)\) is the cost function. <br>
                    &nbsp; This equation shows that at each iteration, the gradient moves in the opposite of the
                    gradient (negative sign).
                    <br><br>

                    <b>Stochastic GD</b><br>
                    &nbsp; • Instead of using all the data like normal GD, computes the gradient using a single sample
                    at each step. <br>
                    &nbsp; • More time efficient

                    <br><br>
                    <b>Batch GD</b><br>
                    &nbsp; • Uses fixed small number of batches of data points per time step <br>
                    &nbsp; • More time efficient <br>
                    &nbsp; • Avoid getting stuck at a local minimum or saddle point




                </p>
                <script>
                    const gdB = document.getElementById('gd-button');
                    const gdP = document.getElementById('gd-paragraph');

                    gdB.addEventListener('click', () => {
                        if (gdP.style.display === 'none') {
                            gdP.style.display = 'block';
                        } else {
                            gdP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />

                <!--Button-->
                <button id="heteroscedasticity-button">heteroscedasticity</button>
                <p id="heteroscedasticity-paragraph" style="display: none;">
                    Residuals are NOT identically distributed; the variance of residuals is not constant. This causes
                    bias in parameter estimates for linear regression and similar models.
                    <br><br>
                    Plot the residuals against independent variable to help visualize if there is heteroscedasticity.

                </p>
                <script>
                    const heteroscedasticityB = document.getElementById('heteroscedasticity-button');
                    const heteroscedasticityP = document.getElementById('heteroscedasticity-paragraph');

                    heteroscedasticityB.addEventListener('click', () => {
                        if (heteroscedasticityP.style.display === 'none') {
                            heteroscedasticityP.style.display = 'block';
                        } else {
                            heteroscedasticityP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="htuning-button">hyperparameter tuning</button>
                <p id="htuning-paragraph" style="display: none;">Process to find the optimal hyperparameters for a
                    model.

                    <br><br>

                    <b>Grid Search</b><br>
                    &nbsp; • Form a grid that is a cartesian product of parameters <br>
                    &nbsp; • Try all combinations to see which yields the best results <br>
                    &nbsp; • Computationally expensive
                    <br><br>
                    <b>Random Search</b><br>
                    &nbsp; • Define distribution for each parameter <br>
                    &nbsp; • Randomly sample from the joint distribution over all parameters

                </p>
                <script>
                    const htuningB = document.getElementById('htuning-button');
                    const htuningP = document.getElementById('htuning-paragraph');

                    htuningB.addEventListener('click', () => {
                        if (htuningP.style.display === 'none') {
                            htuningP.style.display = 'block';
                        } else {
                            htuningP.style.display = 'none';
                        }
                    });
                </script>
                <br />
                <br />


                <!--Button-->
                <button id="ig-button">information gain</button>
                <p id="ig-paragraph" style="display: none;"> In a decision tree, maximize IG recursively on all branches
                    <br>
                    For some feature \(X\) on which we want to split, \( IG(X,Y) = H(Y) - H(Y|X)\) <br>
                    \(IG(X,Y)\) is the reduction in uncertainty in \(Y\) by splitting on \(X\)<br>
                    \(H(Y)\) is the entropy from initial training labels
                </p>
                <script>
                    const igB = document.getElementById('ig-button');
                    const igP = document.getElementById('ig-paragraph');

                    igB.addEventListener('click', () => {
                        if (igP.style.display === 'none') {
                            igP.style.display = 'block';
                        } else {
                            igP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />

                <!--Button-->
                <button id="learning-button">learning curve</button>
                <p id="learning-paragraph" style="display: none;"> Plots model learning performance over time. Y-axis is
                    an evaluation metric (e.g., accuracy, F1 score, etc.). X-axis is iterations over time.
                    <br><br>
                    The model is overfitted if the validation error does not improve over time. If both training and
                    validation errors do not improve over time, then the model is underfitted.
                </p>
                <script>
                    const learningB = document.getElementById('learning-button');
                    const learningP = document.getElementById('learning-paragraph');

                    learningB.addEventListener('click', () => {
                        if (learningP.style.display === 'none') {
                            learningP.style.display = 'block';
                        } else {
                            learningP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="lime-button">LIME</button>
                <p id="lime-paragraph" style="display: none;"> Local Interpretable Model-Agnostic Explanation <br><br>
                    Uses sparse linear models built around various predictions to understand how any model performs in
                    that local vicinity.
                </p>
                <script>
                    const limeB = document.getElementById('lime-button');
                    const limeP = document.getElementById('lime-paragraph');

                    limeB.addEventListener('click', () => {
                        if (limeP.style.display === 'none') {
                            limeP.style.display = 'block';
                        } else {
                            limeP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="logreg-button">logistic regression</button>
                <p id="logreg-paragraph" style="display: none;">
                    A supervised machine learning algorithm for binary classification.
                    It predicts the likelihood of a binary outcome based on input variables using a logistic (sigmoid)
                    function to map linear outputs to probabilities. <br>
                    Sigmoid \( S(x) = \frac{1}{1+e^{-x \beta}} \) <br>
                    &nbsp; where \(x\) are predictor variables <br>
                    &nbsp; and \(\beta\) is a vector of weights <br>
                    <br>
                    \(P (\hat{Y} = 1|X) = S(X \beta) \)

                    <br><br>
                    Pros: <br>
                    &nbsp; • high explainability <br>
                    &nbsp; • quick to compute
                    <br><br>
                    Cons: <br>
                    &nbsp; • high bias, low variance (prone to underfit, assumes linear boundary layer) <br>
                    &nbsp; • weights, \(\beta \) terms, are not accurate if input variables are correlated
                </p>
                <script>
                    const logregB = document.getElementById('logreg-button');
                    const logregP = document.getElementById('logreg-paragraph');

                    logregB.addEventListener('click', () => {
                        if (logregP.style.display === 'none') {
                            logregP.style.display = 'block';
                        } else {
                            logregP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />


                <!--Button-->
                <button id="mae-button">mean absolute error (MAE)</button>
                <p id="mae-paragraph" style="display: none;">
                    Evaluation metric for regression that measures the average
                    absolute difference between the actual and predicted values (the resideuals). Lower MAE means better
                    model
                    performance.
                    \[ MAE = \frac{1}{n} \sum_{i=1}^{n} \mid y_i - \hat{y}_i \mid \]
                </p>
                <script>
                    const maeButton = document.getElementById('mae-button');
                    const maePara = document.getElementById('mae-paragraph');
                    maeButton.addEventListener('click', () => {
                        if (maePara.style.display === 'none') {
                            maePara.style.display = 'block';
                            // Trigger MathJax typesetting after displaying the equation
                            MathJax.typeset();
                        } else {
                            maePara.style.display = 'none';
                        }
                    });
                </script>


                <!--Button-->
                <button id="mse-button">mean squared error (MSE)</button>
                <p id="mse-paragraph" style="display: none;">
                    Evaluation metric for regression that measures the average of the squared differences between
                    actual and predicted values (the variance of residuals).
                    <br>
                    Lower MSE means better model performance. The MSE represents
                    unexplained
                    variance, the portion of variability in the dependent variable that is not explained by the
                    model's independent variables. Goodness of fit metric for liear regression.
                    \[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]

                    MSE is more sensitive to outliers than MAE because it penalizes large errors more.
                </p>
                <script>
                    const mseButton = document.getElementById('mse-button');
                    const msePara = document.getElementById('mse-paragraph');
                    mseButton.addEventListener('click', () => {
                        if (msePara.style.display === 'none') {
                            msePara.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            msePara.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="multicollinearity-button">multicollinearity</button>
                <p id="multicollinearity-paragraph" style="display: none;">
                    When predictors are highly correlated with each other so it's hard to distinguish the true
                    underlying weights. <br><br>
                    Multicollinearity can be assessed with the Variance Inflation Factor (VIF). It quantifies how much
                    the estimated coefficients are inflated when multicollinearity exists. A VIF of 1 means there is no
                    correlated. >1 means there is correlation. 5-10 means there is high multicollinearity. <br><br>
                    Multicollinearity can be removed by: <br>
                    • removing correlated variables<br>
                    • linearly combine variables<br>
                    • PCA (dimensionality reduction)<br>
                    • PLS (partial least squares) <br>
                    • regularization methods (ridge or lasso)
                </p>
                <script>
                    const multicollinearityB = document.getElementById('multicollinearity-button');
                    const multicollinearityP = document.getElementById('multicollinearity-paragraph');
                    multicollinearityB.addEventListener('click', () => {
                        if (multicollinearityP.style.display === 'none') {
                            multicollinearityP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            multicollinearityP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />


                <!--Button-->
                <button id="naive-button">naive bayes</button>
                <p id="naive-paragraph" style="display: none;">
                    A supervised machine learning algorithm used for classification problems that applies Bayes' Theorem
                    with a naive assumption of independence between features.
                    In other words, it decouples the class conditional feature distributions by assuming that the
                    presence of a particular feature in a class is independent of all other features. Each feature's
                    distribution can be independently estimated as a 1-D distribution \( P(X_1...X_n|Y)=\prod_{i=1}^n
                    P(X_i|Y) \). Apply the conditional independence assumption and Bayes' Theorem gives the
                    classification rule \( \hat{y}=argmax_{y_i} P(Y=y_i) \prod_j P(X_j|Y=y_i)\).
                    <br><br>
                    For any machine learning model with <i>k</i> features, there are 2\(^k\) possible feature
                    interactions. You would need 2\(^k\) data points for a good model. However, the conditional
                    independence assumption of Naive Bayes requires only <i>k</i> data points.
                    <br><br>
                    Assumptions \(P(Y|X)\):<br>
                    &nbsp; • Each \(x_i\) is independent of any other \(x_j\) given \(Y\) for any pair of features
                    \(x_i\) and \(x_j\)<br>
                    &nbsp;&nbsp;&nbsp;&nbsp; This is generally not true because features tend to be correlated. <br>
                    &nbsp; • Each feature is given the same weight
                    <br><br>
                    Pros: <br>
                    &nbsp; • Requires a small amount of training data<br>
                    &nbsp; • Computationally fast <br>
                    &nbsp; • Scalable


                </p>
                <script>
                    const naiveB = document.getElementById('naive-button');
                    const naiveP = document.getElementById('naive-paragraph');
                    naiveB.addEventListener('click', () => {
                        if (naiveP.style.display === 'none') {
                            naiveP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            naiveP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />

                <!-- Button -->
                <button id="precision-button">precision</button>
                <p id="precision-paragraph" style="display: none;">
                    A classification model evaluation metric. \( \text{Precision} = \frac{\text{# true
                    positives}}{\text{(# true positives + # false positives)}} \)
                    <br>tradeoff with Recall


                </p>
                <script>
                    const precisionButton = document.getElementById('precision-button');
                    const precisionPara = document.getElementById('precision-paragraph');
                    precisionButton.addEventListener('click', () => {
                        if (precisionPara.style.display === 'none') {
                            precisionPara.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            precisionPara.style.display = 'none';
                        }
                    });
                </script>

                <!-- Button -->
                <button id="pca-button">principal component analysis</button>
                <p id="pca-paragraph" style="display: none;">
                    PCA combines highly correlated data into principal components which capture the most variance.
                    It looks for linear combos of each row vector to explain variance in \(X\).
                    <br><br>

                    1. Find component with max variance <br>
                    2. Find 2nd component uncorrelated to the 1st with 2nd highest variance <br>
                    3. Some <i>k</i> dimensions such that y\(_1\),…,y\(_k\) explains the majority of <i>k</i> variance,
                    <i>k</i> \(<\)\(<\)\(<\) <i>p</i> <br>
                        4. Eigendecomposition of covariance matrix of \(X\)
                        <br><br>
                        Eigenvectors are the principal components. Eigenvalues are the magnitude of variance.
                        <br><br>
                        Find \(\vec{w}\) such that \(y_i = w_i^T x = \sum_{j=1}^p w_{ij}x_j \)

                        <br><br>
                        Assumes variables have a linear relationship.
                        <br><br>
                        Cons: <br>
                        &nbsp; • Struggles with outliers<br>
                        &nbsp; • Sensitive to units for input features (need to standardize)


                </p>
                <script>
                    const pcaB = document.getElementById('pca-button');
                    const pcaP = document.getElementById('pca-paragraph');
                    pcaB.addEventListener('click', () => {
                        if (pcaP.style.display === 'none') {
                            pcaP.style.display = 'block';
                            MathJax.typeset();
                        } else {
                            pcaP.style.display = 'none';
                        }
                    });
                </script>


                <br />
                <br />

                <!--Button-->
                <button id="qq-button">qq plot</button>
                <p id="qq-paragraph" style="display: none;">
                    A QQ (Quantile-Quantile) plot is a graphical tool used to assess if a dataset follows a particular
                    theoretical distribution, such as a normal distribution. This is important because many statistical
                    tests and machine learning algorithms assume normality of the data.
                    <br><br>
                    Quantiles are points taken at regular intervals from the cumulative distribution function (CDF) of a
                    distribution. For a normal distribution, these quantiles would correspond to z-scores.
                    <br><br>
                    1. Generate theoretical quantiles for the distribution you're comparing against (e.g., normal
                    distribution). <br>
                    2. Sort the data and compute the quantiles from the sample (standardized residuals). <br>
                    3. Plot the sample quantiles on the y-axis against the theoretical quantiles on the x-axis.
                    <br><br>
                    Interpretation: <br>
                    &nbsp; • Straight Line: If the data follows the theoretical distribution, the points will lie
                    approximately on a straight line. <br>
                    &nbsp; • Deviation from Line: Systematic deviations from the line indicate departures from the
                    theoretical distribution.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; • Upward/Downward Curvature: This indicates skewness
                    (asymmetry).<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; • S-Shaped Curve: This indicates heavier or lighter tails
                    (kurtosis) than the theoretical distribution.
                </p>
                <script>
                    const qqB = document.getElementById('qq-button');
                    const qqP = document.getElementById('qq-paragraph');
                    qqB.addEventListener('click', () => {
                        if (qqP.style.display === 'none') {
                            qqP.style.display = 'block';
                        } else {
                            qqP.style.display = 'none';
                        }
                    });
                </script>

                <br />
                <br />


                <!--Button-->
                <button id="rf-button">random forest</button>
                <p id="rf-paragraph" style="display: none;">
                    Random forest is an ensemble bagging method that averages decision trees. A random subset of
                    features is considered for each iteration. Its advantages are that it helps overcome decision trees'
                    proneness to overfitting, has quicker training time, and more interpretable.
                </p>
                <script>
                    const rfB = document.getElementById('rf-button');
                    const rfP = document.getElementById('rf-paragraph');
                    rfB.addEventListener('click', () => {
                        if (rfP.style.display === 'none') {
                            rfP.style.display = 'block';
                        } else {
                            rfP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="reg-button">regression</button>
                <p id="reg-paragraph" style="display: none;">A technique for quantifying the relationship
                    between independent variables (features or predictors) and a dependent variable (target or outcome)
                    to make predictions and illuminate the influence of the independent variables on the target.
                    <br />
                    <br />
                    <b>Multiple Regression</b>: Two or more independent variables to predict one dependent variable
                    <br />
                    <b>Linear Regression</b>: The relationship between the independent and dependent
                    variables are fitted with a linear equation <br />
                    <b>Polynominal Regression</b>: The relationship between the independent and
                    dependent variables are fitted with a polynomial equation such as quadratic or cubic <br />
                    <b>Ridge and Lasso Regression</b>: Regularization technique to handle collinearity (high
                    correlation
                    among features) to prevent overfitting. It introduces additional information to penalize extreme
                    parameter (weight) values
                </p>
                <script>
                    const regButton = document.getElementById('reg-button');
                    const regParagraph = document.getElementById('reg-paragraph');
                    regButton.addEventListener('click', () => {
                        if (regParagraph.style.display === 'none') {
                            regParagraph.style.display = 'block';
                        } else {
                            regParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="regularization-button">regularization</button>
                <p id="regularization-paragraph" style="display: none;">
                    Regularization is a technique used to prevent overfitting (reduce complexity) and improve the
                    generalization of models. It aims to reduce variance while only slightly increasing bias.
                    It does so by adding a penalty to the loss function of a model, discouraging it from fitting the
                    training data too closely (shrinks coefficient of features) and instead promoting simpler models
                    that generalize better to unseen data.

                    <br><br>
                    <b>L1 (Lasso) Regularization</b><br>
                    &nbsp; • Penalty: absolute value of coefficient to the objective function <br>
                    &nbsp; • Can be used to feature selection because stricter than L2 (coefficient \(\rightarrow\) 0)
                    <br><br>
                    <b>L2 (Ridge) Regularization</b><br>
                    &nbsp; • Penalty: squared magnitude of the coefficient
                    <br><br>
                    <b>Elastic Net</b><br>
                    &nbsp; • Linear combination of L1 and L2

                </p>
                <script>
                    const regularizationB = document.getElementById('regularization-button');
                    const regularizationP = document.getElementById('regularization-paragraph');
                    regularizationB.addEventListener('click', () => {
                        if (regularizationP.style.display === 'none') {
                            regularizationP.style.display = 'block';
                        } else {
                            regularizationP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="rss-button">residual sum of squares</button>
                <p id="rss-paragraph" style="display: none;">Unexplained variance from a model. For example, in a linear
                    regression, parameter \(\beta\) is found by minimizing RSS. Also known as Sum of Square Errors
                    (SSE). <br>
                    \(RSS(\beta) = (y - X\beta)^T(y-X\beta) \)
                </p>
                <script>
                    const rssB = document.getElementById('rss-button');
                    const rssP = document.getElementById('rss-paragraph');
                    rssB.addEventListener('click', () => {
                        if (rssP.style.display === 'none') {
                            rssP.style.display = 'block';
                        } else {
                            rssP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="rocauc-button">ROC curve & AUC</button>
                <p id="rocauc-paragraph" style="display: none;">
                    The receiver operating characteristic (ROC) curve shows a classification model's performance across
                    all classification thresholds. The area under the curve (AUC) is the area under the ROC. An AUC of 1
                    means the model performed perfectly. An AUC of 0.5 means that the model performed as well as random
                    guessing.
                    <img class="img-fluid z-dept-1" src="images/roc_auc.png" alt="roc_auc.png" style="width: 90%;" />

                </p>
                <script>
                    const rocaucButton = document.getElementById('rocauc-button');
                    const rocaucParagraph = document.getElementById('rocauc-paragraph');

                    rocaucButton.addEventListener('click', () => {
                        if (rocaucParagraph.style.display === 'none') {
                            rocaucParagraph.style.display = 'block';
                        } else {
                            rocaucParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="rmse-button">root mean squared error (RMSE)</button>
                <p id="rmse-paragraph" style="display: none;">The square root of MSE. This metric helps with:
                    <br><br>
                    <b>Interpretability</b>: Because RMSE is expressed in the same unit as the dependent variable,
                    it is easier
                    to interpret and more relatable to the original scale of the data.<br><br>

                    <b>Outlier Sensitivity</b>: RMSE is more sensitive to outliers than MSE, as larger errors
                    contribute more
                    significantly due to the squared and square root operations.<br><br>

                    <b>Direct Comparison</b>: RMSE offers a direct comparison to the standard deviation of the
                    target.
                    If the
                    RMSE is close to the standard deviation (the inherent variability in the original data), then it
                    indicates that the model's predictions are capturing a similar level of variability on average
                    as
                    the observed data. <br><br>

                    <b>Balanced Assessment</b>: By taking the square root, RMSE balances the impact of extreme
                    errors,
                    offering
                    a more well-rounded evaluation of prediction accuracy in regression models.
                </p>
                <script>
                    const rmse_Button = document.getElementById('rmse-button');
                    const rmse_Paragraph = document.getElementById('rmse-paragraph');
                    rmse_Button.addEventListener('click', () => {
                        if (rmse_Paragraph.style.display === 'none') {
                            rmse_Paragraph.style.display = 'block';
                        } else {
                            rmse_Paragraph.style.display = 'none';
                        }
                    });
                </script>
                <br />
                <br />


                <!--Button-->
                <button id="sensitivity-button">sensitivity</button>
                <p id="sensitivity-paragraph" style="display: none;">A classification model evaluation metric also known
                    as <i>recall</i> or the <i>true positive rate</i>. \( \text{Sensitivity} = \frac{\text{# true
                    positives}}{\text{(# true positives + # false negatives)}} \)
                </p>
                <script>
                    const sensitivityButton = document.getElementById('sensitivity-button');
                    const sensitivityParagraph = document.getElementById('sensitivity-paragraph');
                    sensitivityButton.addEventListener('click', () => {
                        if (sensitivityParagraph.style.display === 'none') {
                            sensitivityParagraph.style.display = 'block';
                        } else {
                            sensitivityParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="shap-button">SHAP</button>
                <p id="shap-paragraph" style="display: none;">Shapley Additive Explanation
                    <br><br>
                    SHAP values show the average marginal contribution of a feature over all possible combinations of
                    inputs.
                </p>
                <script>
                    const shapB = document.getElementById('shap-button');
                    const shapP = document.getElementById('shap-paragraph');
                    shapB.addEventListener('click', () => {
                        if (shapP.style.display === 'none') {
                            shapP.style.display = 'block';
                        } else {
                            shapP.style.display = 'none';
                        }
                    });
                </script>


                <!--Button-->
                <button id="specificity-button">specificity</button>
                <p id="specificity-paragraph" style="display: none;">A classification model evaluation metric. \(
                    \text{Specificity} = \frac{\text{# false positives}}{\text{(# false positives + # true negatives)}}
                    \)
                </p>
                <script>
                    const specificityButton = document.getElementById('specificity-button');
                    const specificityParagraph = document.getElementById('specificity-paragraph');
                    specificityButton.addEventListener('click', () => {
                        if (specificityParagraph.style.display === 'none') {
                            specificityParagraph.style.display = 'block';
                        } else {
                            specificityParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="sst-button">SST (total sum of squares)</button>
                <p id="sst-paragraph" style="display: none;">The sum of the squared differences between each
                    observation
                    and the mean of the dependent variable \( \hat{y} \).
                    <img class="img-fluid z-dept-1" src="images/sst.jpg" alt="sst.jpg" style="width: 70%;" />
                </p>
                <script>
                    const sstButton = document.getElementById('sst-button');
                    const sstParagraph = document.getElementById('sst-paragraph');

                    sstButton.addEventListener('click', () => {
                        if (sstParagraph.style.display === 'none') {
                            sstParagraph.style.display = 'block';
                        } else {
                            sstParagraph.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="selection-button">subset selection</button>
                <p id="selection-paragraph" style="display: none;">
                    Stepwise Selection <br>
                    &nbsp; • <b>Forward</b>: Start with an empty model and add the most useful predictors iteratively
                    <br>
                    &nbsp; • <b>Backward</b>: Start with a full model and remove least useful predictors iteratively
                </p>
                <script>
                    const selectionB = document.getElementById('selection-button');
                    const selectionP = document.getElementById('selection-paragraph');

                    selectionB.addEventListener('click', () => {
                        if (selectionP.style.display === 'none') {
                            selectionP.style.display = 'block';
                        } else {
                            selectionP.style.display = 'none';
                        }
                    });
                </script>

                <!--Button-->
                <button id="svm-button">support vector machines</button>
                <p id="svm-paragraph" style="display: none;">
                    A max-margin supervised classification model that forms a hyperplane that linearly separates the
                    training data. The margin between the decision boundary (hyperplane) to any training point (usually
                    the support vectors) is maximized. The margin can be linear or nonlinear. The points closest to the
                    hyperplane that define the margins are support vectors.<br><br>

                    SVM works well: <br>
                    • for high dimensional spaces<br>
                    • when there is a clear hyperplane <br>
                    • for nonlinear decision boundary <br><br>

                    SVM does not work well: <br>
                    • on large datasets (high computational complexity)<br>
                    • when target classes overlap (no clear decision boundaries)<br>
                    • SVM is also hard to interpret/explain and is sensitive to outliers

                </p>
                <script>
                    const svmB = document.getElementById('svm-button');
                    const svmP = document.getElementById('svm-paragraph');

                    svmB.addEventListener('click', () => {
                        if (svmP.style.display === 'none') {
                            svmP.style.display = 'block';
                        } else {
                            svmP.style.display = 'none';
                        }
                    });
                </script>
                <br />
                <br />

                <!--Button-->
                <button id="xgb-button">XGBoost</button>
                <p id="xgb-paragraph" style="display: none;">
                    eXtreme Gradient Boosting (XGBoost) is a supervised ensemble method that uses sequential boosting,
                    combining multiple weak learners (often decision trees) sequentially to create a strong learner.
                    It is the generalized form of AdaBoost.
                    It uses gradients to identify the shortcomings of previous models rather than high weight points
                    (like AdaBoost). All classifiers have equal weights in XGBoost. High execution speed and model
                    performance.

                </p>
                <script>
                    const xgbB = document.getElementById('xgb-button');
                    const xgbP = document.getElementById('xgb-paragraph');

                    xgbB.addEventListener('click', () => {
                        if (xgbP.style.display === 'none') {
                            xgbP.style.display = 'block';
                        } else {
                            xgbP.style.display = 'none';
                        }
                    });
                </script>



                <br />
                <!-- <hr> -->
                <!-- Adds related posts to the end of an article -->
                <!-- <h3 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h3>
                <p class="mb-2">Here are some more articles you might like to read next:</p>
                <li class="my-2"> -->
                <!-- <a class="text-gray-700 underline font-semibold hover:text-gray-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> -->
                </li>

            </article>
        </div>
    </div>

    <!-- Footer -->
    <footer class="fixed-bottom">
        <div class="container mt-0">
            <!-- © Copyright 2022 Eunice Koid. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. -->

        </div>
    </footer>

    <!-- JavaScripts -->
    <script>
        function toggleNavigation() {
            var navigation = document.querySelector('.fixedNavigation');
            if (window.innerWidth <= 1400) {
                navigation.style.display = 'none';
            } else {
                navigation.style.display = 'block';
            }
        }

        // Initial call to set the initial state on page load
        toggleNavigation();

        // Attach the function to the resize event
        window.addEventListener('resize', toggleNavigation);
    </script>

    <!-- jQuery -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
        integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js"
        integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js"
        integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js"
        integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
    <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"
        integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <script defer src="/assets/js/masonry.js" type="text/javascript"></script>

    <!-- Medium Zoom JS -->
    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"
        integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
    <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
    <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
    <script type="text/javascript">
        window.MathJax = {
            tex: {
                tags: 'ams'
            }
        };
    </script>
    <script defer type="text/javascript" id="MathJax-script"
        src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</body>

</html>